{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k4fnzJJDo60Y"
   },
   "source": [
    "# Lab Deep Learning / Multi-Layer Perceptron for binary-classification / in pytorch\n",
    "\n",
    "**Author: geoffroy.peeters@telecom-paris.fr**\n",
    "\n",
    "For any remark or suggestion, please feel free to contact me.\n",
    "\n",
    "\n",
    "\n",
    "## Objective:\n",
    "\n",
    "The objective of this lab is to develop a two hidden layers MLP to perform **binary classification**.\n",
    "\n",
    "We will use a MLP with 2 hidden layer with $n_{h1}=20$ and $n_{h2}=10$ hidden units and ```relu``` activation functions.\n",
    "You will perform 1000 iterations (epochs) of SGD to find the parameters.\n",
    "\n",
    "Note: for this lab, we do not separate the dataset into a train, validation and test part.\n",
    "\n",
    "### Data normalization\n",
    "\n",
    "You should normalize the data to zero mean and unit standard deviation\n",
    "\n",
    "### Model\n",
    "\n",
    "There are various ways to write NN model in pytorch. \n",
    "\n",
    "In this lab, you will write three different implementations:\n",
    "- **Model A**: manually defining the parameters (W1,b1,W2,b2,W3,b3), writing the forward equations, writting the loss equation, calling the .backward() and manually updating the weights using W1.grad. You will write the loop to perform 1000 epochs.\n",
    "- **Model B**: using the Sequential class of pytorch\n",
    "- **Model C**: a custom torch.nn.Module class for this.\n",
    "\n",
    "For Model B and C, you will use the ready made loss and optimization from the nn and optim packages. You can use the same code to optimize the parameters of Model B and C.\n",
    "\n",
    "### Loss\n",
    "\n",
    "Since we are dealing with a binary classification problem, we will use a Binary Cross Entropy loss (use ```torch.nn.BCELoss``` for Model B and C).\n",
    "\n",
    "### Parameters update/ Optimization\n",
    "\n",
    "For updating the parameters, we will use as optimizer a simple SGD algorithm (use ```torch.optim.SGD``` for Model B and C) with a learning rate of 0.1.\n",
    "\n",
    "Don't forget that an optimizer is applied to a set of parameters (```my_model.parameters()``` gives the parameters of the network for Model B and C).\n",
    "Once the gradients have been computed (after the backpropagation has been performed), you can perform one step of optimization (using ```optimizer.step()``` for Model B and C).\n",
    "\n",
    "### Backward propagation\n",
    "\n",
    "Backpropagation is automatically performed in pytorch using the ```autograd``` package. \n",
    "First, reset the gradients of all parameters (using ```optimizer.zero_grad()``` for Model B and C), then perform the backpropagation ```loss.backward()```. \n",
    "\n",
    "## Your task:\n",
    "\n",
    "You need to add the missing parts in the code (parts between ```# --- START CODE HERE``` and ```# --- END CODE HERE```)\n",
    "\n",
    "## Documentation:\n",
    "- NN: https://pytorch.org/docs/stable/nn.html\n",
    "- Autograd: https://pytorch.org/docs/stable/autograd.html\n",
    "- Optim: https://pytorch.org/docs/stable/optim.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tuvU8y2Lo60Z"
   },
   "source": [
    "## Load the python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I1VTuwVio60a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "student = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Zof__thjo60d",
    "outputId": "76611943-5207-4c11-cd77-4811e5d6282e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bsb-phrJo60g"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We take the usual circle dataset from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Otam7ukPo60g"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X_np, y_np = datasets.make_circles(n_samples=1000, noise=0.2, factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5idAV4Co60i"
   },
   "source": [
    "We convert the ```numpy tensors``` to ```torch tensors```. \n",
    "The difference being that the latters allows to do automatic gradient differentiation (back-propagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rPxnzVSDo60j"
   },
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X_np).float()\n",
    "y = torch.from_numpy(y_np).float()\n",
    "y = y.view(len(y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "YHgd8JYPo60l",
    "outputId": "748d010f-c710-473c-9008-531e8106ff50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2])\n",
      "torch.Size([1000, 1])\n",
      "tensor([-0.0029,  0.0115])\n",
      "tensor([0.5984, 0.5895])\n"
     ]
    }
   ],
   "source": [
    "print(X.size())\n",
    "print(y.size())\n",
    "print(X.mean(dim=0))\n",
    "print(X.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Unp-3kjjo60n"
   },
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "6rIwFaauo60n",
    "outputId": "ac0930fd-21e6-417f-a7f7-5b4dd487884c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.3842e-09, -7.6294e-09])\n",
      "tensor([1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "X -= X.mean(dim=0)\n",
    "X /= X.std(dim=0)\n",
    "print(X.mean(dim=0))\n",
    "print(X.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rc3VgVWOo60p"
   },
   "source": [
    "## Definition of the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OrBQMqCJo60r"
   },
   "outputs": [],
   "source": [
    "n_in = X.shape[1]\n",
    "n_h1 = 20\n",
    "n_h2 = 10\n",
    "n_out = 1\n",
    "\n",
    "nb_epoch = 10000\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CIzjntgZo60t"
   },
   "source": [
    "## Model 1 (writing the network equations)\n",
    "\n",
    "Here, you will define the variables and write the equations of the network yourself (as you would do in numpy).\n",
    "However you will use ```torch tensors``` instead of ```numpy array```. \n",
    "\n",
    "***Why ?*** because torch tensors will allows you to automatically get the gradient. You will use ```loss.backward``` to launch the backpropagation from ```loss```. Then, for all tensors you created and for which you declared ```requires_grad=True```, you will get the gradient of ```loss```with respect to this variable in the field ```.grad```. \n",
    "\n",
    "***Example*** ```W1 = torch.tensors(..., requires_grad=True)``` ... ```loss.backward``` will have the gradient $\\frac{d Loss}{d W1}$in ```W1.grad```.\n",
    "\n",
    "Don't forget that the weight $W_1, W_2, \\cdots$ matrices should be initialized randomly with small values; while the bias vectors $b_1, b_2, \\cdots$can be initialized to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "id": "HB65rqW4o60u",
    "outputId": "65829bca-0b23-48c6-e72b-159181c2fb31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 3.0148777961730957\n",
      "epoch 500, loss 0.26418641209602356\n",
      "epoch 1000, loss 0.2596597969532013\n",
      "epoch 1500, loss 0.2579493820667267\n",
      "epoch 2000, loss 0.25669705867767334\n",
      "epoch 2500, loss 0.25527313351631165\n",
      "epoch 3000, loss 0.25481417775154114\n",
      "epoch 3500, loss 0.25432372093200684\n",
      "epoch 4000, loss 0.25400373339653015\n",
      "epoch 4500, loss 0.2537267506122589\n",
      "epoch 5000, loss 0.25350072979927063\n",
      "epoch 5500, loss 0.2533334195613861\n",
      "epoch 6000, loss 0.25316059589385986\n",
      "epoch 6500, loss 0.25298988819122314\n",
      "epoch 7000, loss 0.25285691022872925\n",
      "epoch 7500, loss 0.2527748942375183\n",
      "epoch 8000, loss 0.25266438722610474\n",
      "epoch 8500, loss 0.2528286874294281\n",
      "epoch 9000, loss 0.2527005076408386\n",
      "epoch 9500, loss 0.252716988325119\n"
     ]
    }
   ],
   "source": [
    "# --- We first initialize the variables of the network (W1, b1, ...)\n",
    "if student:\n",
    "    # --- START CODE HERE (01)\n",
    "    W1 = torch.randn(n_in,n_h1, requires_grad = True)\n",
    "   # print(W1)\n",
    "   \n",
    "    b1 = torch.zeros(1,n_h1,requires_grad = True)\n",
    "    #b1.requires_grad = True \n",
    "    \n",
    "    W2 = torch.randn(n_h1,n_h2,requires_grad = True)\n",
    "   # W2.requires_grad = True\n",
    "    \n",
    "    b2 = torch.zeros(1,n_h2,requires_grad = True)\n",
    "   # b2.requires_grad = True \n",
    "    \n",
    "    W3 = torch.randn(n_h2,n_out,requires_grad = True)\n",
    "  #  W3.requires_grad = True\n",
    "  \n",
    "    b3 = torch.zeros(1,n_out,requires_grad = True)\n",
    "    #b3.requires_grad = True \n",
    "    \n",
    " \n",
    "    # --- END CODE HERE\n",
    "\n",
    "# --- We then write a function to perform the forward pass (using pytorch opertaors, not numpy operators)\n",
    "# --- taking X as input and returing hat_y as output\n",
    "\n",
    "\n",
    "def model(X):\n",
    "    if student:\n",
    "        # --- START CODE HERE (02)\n",
    "        A0 = X\n",
    "        Z1 = torch.mm(A0, W1) + b1\n",
    "        A1 = torch.relu(Z1)\n",
    "        Z2 = torch.mm(A1,W2)+b2\n",
    "        A2 = torch.relu(Z2)\n",
    "        Z3 = torch.mm(A2,W3)+b3\n",
    "        A3 = torch.sigmoid(Z3)\n",
    "        hat_y = A3\n",
    "        #print (hat_y)\n",
    "        # --- END CODE HERE\n",
    "       \n",
    "        return hat_y\n",
    "\n",
    "# --- We then iterate over epochs (we do not perform split into mini-batch here)\n",
    "# --- For each iteration, we\n",
    "# ---   a) perform the forward pass, \n",
    "# ---   b) compute the loss/cost, \n",
    "# ---   c) compute the backward pass to get the gradients of the cost w.r.t. the parameters W1, b1, ...\n",
    "# ---   d) perform the update of the parameters W1, b1, ...\n",
    "for num_epoch in range(0, nb_epoch):    \n",
    "\n",
    "    # --- a) Forward pass: X (n_in, N), hat_y (n_out, N)\n",
    "    hat_y = model(X)\n",
    "    #print (hat_y)\n",
    "    # -- We clip hat_y in order to avoid log(0)\n",
    "    eps = 1e-10\n",
    "    hat_y = torch.clamp(hat_y, eps, 1-eps)\n",
    "   \n",
    "    \n",
    "    # --- b) Computing the loss/cost\n",
    "    if student:\n",
    "        # --- START CODE HERE (03)\n",
    "        m=X.shape[0]\n",
    "        loss = - ( torch.mul( torch.log(hat_y),y   )  + torch.mul( torch.log(1-hat_y ),1-y )    )\n",
    "        cost = (1/m)*loss.sum()\n",
    "        # --- END CODE HERE\n",
    "        #print (cost)\n",
    "    \n",
    "    if num_epoch % 500 == 0:\n",
    "        print('epoch {}, loss {}'.format(num_epoch, cost))\n",
    "        \n",
    "    \n",
    "   \n",
    "    # --- c) Backward pass\n",
    "    cost.backward()\n",
    "   \n",
    "    # --- \"with torch.no_grad()\" temporarily set all the requires_grad flag to false\n",
    "    with torch.no_grad():\n",
    "        # --- d) perform the update of the parameters W1, b1, ...\n",
    "        if student:\n",
    "            # --- the gradients dLoss/dW1 is stored in W1.grad, dLoss/db1 is stored in b1.grad, ...\n",
    "            # --- START CODE HERE (04)\n",
    "            W1 -=  alpha* W1.grad\n",
    "            \n",
    "            b1 -= alpha*b1.grad\n",
    "            W2 -=  alpha*W2.grad\n",
    "            b2 -= alpha*b2.grad\n",
    "            W3  -= alpha * W3.grad\n",
    "            b3 -=  alpha * b3.grad\n",
    "            \n",
    "            W1.grad.zero_()\n",
    "            b1.grad.zero_()\n",
    "            W2.grad.zero_()\n",
    "            b2.grad.zero_()\n",
    "            W3.grad.zero_()\n",
    "            b3.grad.zero_()\n",
    "                    # --- END CODE HERE\n",
    "\n",
    "    # --- We need to set to zero all gradients (otherwise they are cumulated)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxgA4DXVo60w"
   },
   "source": [
    "## Model 2 (using nn.sequential)\n",
    "\n",
    "Here, you will use the package ```torch.nn``` which comes with a predefined set of layers. The syntax is close to the one of ```keras```(```Sequential```), but differs in the fact that layers are splitted into the matrix multiplication followed by a non-linear activations (```keras```merge both using the ```Dense```layers).\n",
    "\n",
    "The model created will have all its parameters accessible as a dictionary and can be accessed using ```model.parameters()```. It is therefore a convenient way to write simple sequential networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "elGQpQzjo60x",
    "outputId": "e51c826e-81d7-40de-817a-83f3bf3b1408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=20, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if student:\n",
    "    # --- START CODE HERE (05)\n",
    "    my_model = nn.Sequential(\n",
    "              nn.Linear(n_in,n_h1),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(n_h1,n_h2),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(n_h2,n_out),\n",
    "              nn.Sigmoid()    \n",
    "    )\n",
    "    print(my_model)\n",
    "    # --- END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4twpnbEAo60z"
   },
   "source": [
    "## Model 3 (using a class definition)\n",
    "\n",
    "Here, you will write the network using the recommended pytroch way; i.e. by defining a class.\n",
    "This class inherit from the main class ```torch.nn.Module```.\n",
    "You only need to write the ```__init__``` method and the ```forward``` method.\n",
    "\n",
    "In object programming, the ```__init__``` method defines the attributes of your class. Since the attributes of your  network are the parameters to be trained (weights and biases), you should declare in the ```__init``` all the layers that involve parameters to be trained (mostly the ```Linear```layers which perform the matrix multiplication).\n",
    "\n",
    "The ```forward``` method contains the code of the forward pass itself. It can of course call attributes defined in the ```__init___``` method. It is the method used when calling ```model(x)```.\n",
    "\n",
    "As before, the model created will have all its parameters accessible as a dictionary and can be accessed using ```model.parameters()```. \n",
    "\n",
    "Classes are convenient way to write more complex network than what you can do with ```nn.sequential```. Note that you can actually include a ```nn.sequential``` in your class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNsNkq9Do60z"
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_h1, n_h2, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        if student:\n",
    "            # --- START CODE HERE (06)\n",
    "            self.fc1 = nn.Linear(n_in,n_h1) # hidden layer 1\n",
    "            self.fc2 = nn.Linear(n_h1,n_h2) # hidden layer 2\n",
    "            self.fc3 = nn.Linear(n_h2,n_out) # output layer\n",
    "            # --- END CODE HERE\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        if student:\n",
    "            # --- START CODE HERE (07)\n",
    "            A0 = X\n",
    "            A1 = torch.relu(self.fc1(A0))   # activation function for hidden layer 1\n",
    "            A2 = torch.relu(self.fc2(A1))   # activation function for hidden layer 2\n",
    "            A3 = torch.sigmoid(self.fc3(A2)) # activation function for output layer\n",
    "            # --- END CODE HERE\n",
    "\n",
    "        return A3\n",
    "\n",
    "# --- START CODE HERE\n",
    "my_model = Net(n_in, n_h1, n_h2, n_out)\n",
    "# --- END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ts4aVeIPo601"
   },
   "source": [
    "## Criterion and Optimization for model 2 and model 3\n",
    "\n",
    "The code of Model 1 is self-contained, i.e. it already contains all necessary instruction to perform forawrd, loss, backward and parameter updates.\n",
    "\n",
    "When using ```nn.sequential``` (model 2) or a class definition of the network (model 3), we still need to define \n",
    "- what we will minimize (the loss to be minimized, i.e. Binary-Cross-Entropy). We can of course write the equation of it by hand but pytorch comes with a very large number of pre-build loss functions (within ```torch.nn```)\n",
    "- how we will minimize the loss, i.e. what parameter update algorithms we will use (SGD, momentum). We can of course write the equation of it by hand but pytorch comes with a very large number of pre-build loss functions (within ```torch.nn```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xs63V-Wgo602"
   },
   "outputs": [],
   "source": [
    "if student:\n",
    "    # --- START CODE HERE (08)\n",
    "    criterion = nn.BCELoss ()\n",
    "    optimizer = optim.SGD(my_model.parameters(), lr=alpha, momentum=0.9)\n",
    "    # --- END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qL7ePZi9o604"
   },
   "source": [
    "## Training for model 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XamuBM_ho604"
   },
   "source": [
    "Having defined the network, the citerion to be minimized and the optimizer, we then perform a loop over epochs (iterations); at each step we\n",
    "- compute the forward pass by passing the data to the model: ```haty = model(x)```\n",
    "- compute the the loss (the criterion)\n",
    "- putting at zero the gradients of all the parameters of the network (this is important since, by default, pytorch accumulate the gradients over time)\n",
    "- computing the backpropagation (using as before ```.backward()```)\n",
    "- performing one step of optimization (using ```.step()```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "colab_type": "code",
    "id": "rKfrD8V3o605",
    "outputId": "a46f122a-aed7-4100-a788-af4be72fab60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.6827234029769897\n",
      "epoch 500, loss 0.2653011083602905\n",
      "epoch 1000, loss 0.2631778120994568\n",
      "epoch 1500, loss 0.2625701427459717\n",
      "epoch 2000, loss 0.2619813084602356\n",
      "epoch 2500, loss 0.26154911518096924\n",
      "epoch 3000, loss 0.26114535331726074\n",
      "epoch 3500, loss 0.2607770264148712\n",
      "epoch 4000, loss 0.2601981461048126\n",
      "epoch 4500, loss 0.25940394401550293\n",
      "epoch 5000, loss 0.25771698355674744\n",
      "epoch 5500, loss 0.2557159662246704\n",
      "epoch 6000, loss 0.2540806233882904\n",
      "epoch 6500, loss 0.2531243860721588\n",
      "epoch 7000, loss 0.2504999339580536\n",
      "epoch 7500, loss 0.24931496381759644\n",
      "epoch 8000, loss 0.25237181782722473\n",
      "epoch 8500, loss 0.25752025842666626\n",
      "epoch 9000, loss 0.24771833419799805\n",
      "epoch 9500, loss 0.25716647505760193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0545596198>]"
      ]
     },
     "execution_count": 91,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_l = []\n",
    "for num_epoch in range(nb_epoch):\n",
    "  \n",
    "    if student:\n",
    "        # --- START CODE HERE (09)\n",
    "       \n",
    "        hat_y = my_model(X) # Forward pass: Compute predicted y by passing  x to the model          \n",
    "        optimizer.zero_grad() \n",
    "        #print (y.squeeze())\n",
    "        loss = criterion(hat_y,y) # Compute loss \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Zero gradients, perform a backward pass, and update the weights. \n",
    "        ... # re-init the gradients (otherwise they are cumulated)\n",
    "        ... # perform back-propagation\n",
    "        ... # update the weights\n",
    "        # --- END CODE HERE\n",
    "\n",
    "\n",
    "    loss_l.append(loss)\n",
    "\n",
    "    if num_epoch % 500 == 0:\n",
    "        print('epoch {}, loss {}'.format(num_epoch, loss.item()))\n",
    "        \n",
    "# ----------------\n",
    "plt.plot(loss_l)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tjxIu8dBx3BZ"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "To evaluate the work, you should rate the code for \n",
    "- 1) Model 1: Initialization of W1, b1, ... (01)\n",
    "- 2) Model 1: Forward-pass (02)\n",
    "- 3) Model 1: Loss and Cost computation  (03)\n",
    "- 4) Model 1: Manual update of the parameters (04)\n",
    "- 5) Model 2: using nn.sequential (05)\n",
    "- 6) Model 3: using class definition: __init__ method (06)\n",
    "- 7) Model 3: using class definition: forward method (07)\n",
    "- 8) Model 2 and 3: Loss (criterion) and parameter update algorithms (optimizer) (08)\n",
    "- 9) Model 2 and 3: code inside the loop (09)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab1_saifeddine_ghribi_pytorch_student.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
